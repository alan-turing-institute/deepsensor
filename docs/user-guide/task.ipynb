{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a 'task'?\n",
    "\n",
    "The concept of a *task* is central to DeepSensor.\n",
    "It originates from the meta-learning literature in machine learning and has a specific meaning.\n",
    "\n",
    "Users unfamiliar with the notation and terminology of meta-learning are recommended to expand the section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Click to reveal the meta-learning primer\n",
    ":class: dropdown\n",
    "\n",
    "**Sets of observations**\n",
    "\n",
    "A *set* of observations is a collection of $M$ input-output pairs $\\{(\\mathbf{x}_1, \\mathbf{y}_1), (\\mathbf{x}_2, \\mathbf{y}_2), \\ldots, (\\mathbf{x}_M, \\mathbf{y}_M)\\}$.\n",
    "In DeepSensor $\\mathbf{x}_i \\in \\mathbb{R}^2$ is a 2D spatial location (such as latitude-longitude)\n",
    " and $\\mathbf{y}_i \\in \\mathbb{R}^N$ is an $N$-dimensional observation at that location (such as a temperature and precipitation).\n",
    "Context sets may lie on scattered, off-grid locations (such as weather stations), or on a regular grid (such as a reanalysis or satellite data).\n",
    "A *set* can be compactly written as $(\\mathbf{X}, \\mathbf{Y})$, where $\\mathbf{X} \\in \\mathbb{R}^{2\\times M}$ and $\\mathbf{Y} \\in \\mathbb{R}^{N\\times M}$.\n",
    "\n",
    "**Context sets**\n",
    "\n",
    "A *context set* is a set of observations that are used to make predictions for another set of observations. Following our notations above, we denote a context set as $C_j=(\\mathbf{X}^{(c)}, \\mathbf{Y}^{(c)})_j$.\n",
    "We may have multiple context sets, denoted as $C = \\{ (\\mathbf{X}^{(c)}, \\mathbf{Y}^{(c)})_j \\}_{j=1}^{N_C}$.\n",
    "\n",
    "**Target sets**\n",
    "\n",
    "A *target set* is a set of observations that we wish to predict using the context sets.\n",
    "Similarly to context sets, we denote the collection of all target sets as $T = \\{ (\\mathbf{X}^{(t)}, \\mathbf{Y}^{(t)})_j \\}_{j=1}^{N_T}$.\n",
    "During training, the target observations $\\mathbf{y}_i$ are known, but at inference time will be unknown latent variables.\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "A *task* is a collection of context sets and target sets.\n",
    "We denote a task as $\\mathcal{D} = (C, T)$.\n",
    "The modelling goal is make probabilistic predictions for the target variables $\\mathbf{Y}^{(t)}_j$ given the context sets $C$ and target prediction locations $\\mathbf{X}^{(t)}_j$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DeepSensor Task\n",
    "\n",
    "In DeepSensor, a `Task` is a `dict`-like data structure that contains context sets, target sets, and other metadata.\n",
    "Before diving into the [](./task_loader) class which generates `Task` objects from `xarray` and `pandas` objects,\n",
    "we will first introduce the `Task` class itself.\n",
    "In the code cell below, `task` is a `Task` object.\n",
    "Printing a `Task` will print each of its entries and replace numerical arrays with their shape for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 3124/3124 [02:38<00:00, 19.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.captureWarnings(True)\n",
    "\n",
    "import deepsensor.torch\n",
    "from deepsensor.data import DataProcessor\n",
    "from deepsensor.data.sources import get_ghcnd_station_data, get_era5_reanalysis_data, get_earthenv_auxiliary_data, get_gldas_land_mask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Using the same settings allows use to use pre-downloaded cached data\n",
    "data_range = (\"2016-06-25\", \"2016-06-30\")\n",
    "extent = \"europe\"\n",
    "station_var_IDs = [\"TAVG\", \"PRCP\"]\n",
    "era5_var_IDs = [\"2m_temperature\", \"10m_u_component_of_wind\", \"10m_v_component_of_wind\"]\n",
    "auxiliary_var_IDs = [\"elevation\", \"tpi\"]\n",
    "cache_dir = \"../../.datacache\"\n",
    "\n",
    "station_raw_df = get_ghcnd_station_data(station_var_IDs, extent, date_range=data_range, cache=True, cache_dir=cache_dir)\n",
    "era5_raw_ds = get_era5_reanalysis_data(era5_var_IDs, extent, date_range=data_range, cache=True, cache_dir=cache_dir)\n",
    "auxiliary_raw_ds = get_earthenv_auxiliary_data(auxiliary_var_IDs, extent, \"10KM\", cache=True, cache_dir=cache_dir)\n",
    "land_mask_raw_ds = get_gldas_land_mask(extent, cache=True, cache_dir=cache_dir)\n",
    "\n",
    "data_processor = DataProcessor(x1_name=\"lat\", x2_name=\"lon\")\n",
    "era5_ds = data_processor(era5_raw_ds)\n",
    "aux_ds, land_mask_ds = data_processor([auxiliary_raw_ds, land_mask_raw_ds], method=\"min_max\")\n",
    "station_df = data_processor(station_raw_df)"
   ],
   "metadata": {
    "collapsed": false,
    "tags": [
     "hide-cell"
    ],
    "ExecuteTime": {
     "start_time": "2023-11-01T14:28:15.732009455Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "ExecuteTime": {
     "end_time": "2023-11-01T14:32:15.553656830Z",
     "start_time": "2023-11-01T14:32:15.548454739Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepsensor.data import TaskLoader\n",
    "task_loader = TaskLoader(context=[era5_ds, land_mask_ds], target=station_df)\n",
    "task = task_loader(\"2016-06-25\", context_sampling=[52, 112], target_sampling=245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T14:32:15.566930620Z",
     "start_time": "2023-11-01T14:32:15.553282595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2016-06-25 00:00:00\n",
      "ops: []\n",
      "X_c: [(2, 52), (2, 112)]\n",
      "Y_c: [(3, 52), (1, 112)]\n",
      "X_t: [(2, 245)]\n",
      "Y_t: [(2, 245)]\n"
     ]
    }
   ],
   "source": [
    "print(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task structure\n",
    "\n",
    "A `Task` typically contains at least the following entries:\n",
    "- `\"time\"`: timestamp that was used for slicing the spatiotemporal data.\n",
    "- `\"ops\"` list of processing operations that have been applied to the data (more on this shortly).\n",
    "- `\"X_c\"` and `\"Y_c\"`: length-$N_C$ lists of context set observations $\\mathbf{X}^{(c)}_i \\in \\mathbb{R}^{2\\times M}$ and $\\mathbf{Y}^{(c)}_i \\in \\mathbb{R}^{N\\times M}$.\n",
    "- `\"X_t\"` and `\"Y_t\"`: as above, but for the target sets. In the example above, the target observations are known, so this `Task` may be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Exercise:**\n",
    "\n",
    "For the `task` object above, use the `\"X_c\"`, `\"Y_c\"`, `\"X_t\"`, and `\"Y_t\"` entries to work out the following (answer hidden below):\n",
    "- The number of context sets\n",
    "- The number of observations in each context set\n",
    "- The dimensionality of each context set\n",
    "- The number of target sets\n",
    "- The number of observations in each target set\n",
    "- The dimensionality of each target set\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Click to reveal the answers!\n",
    ":class: dropdown\n",
    "\n",
    "Answers, respectively: 2 context sets, 52 and 112 context observations, 3 and 1 context dimensions, 1 target set, 245 target observations, 2 target dimensions.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gridded data in Tasks\n",
    "\n",
    "For convenience, data that lies on a regular grid is given a compact tuple representation for the `\"X\"` entries:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "task_with_gridded_data = task_loader(\"2016-06-25\", context_sampling=[\"all\", \"all\"], target_sampling=245)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:32:15.620494504Z",
     "start_time": "2023-11-01T14:32:15.570462444Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2016-06-25 00:00:00\n",
      "ops: []\n",
      "X_c: [((1, 141), (1, 221)), ((1, 140), (1, 220))]\n",
      "Y_c: [(3, 141, 221), (1, 140, 220)]\n",
      "X_t: [(2, 245)]\n",
      "Y_t: [(2, 245)]\n"
     ]
    }
   ],
   "source": [
    "print(task_with_gridded_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:32:15.628949091Z",
     "start_time": "2023-11-01T14:32:15.611675646Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above example, the first context set lies on a 141 x 221 grid, and the second context set lies on a 140 x 220 grid."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task methods\n",
    "The `Task` class also contains methods for applying processing operations the data (like removing NaNs, adding batch dimensions, etc.).\n",
    "These operations will be recorded in the order they were applied the `\"ops\"` entry of the `Task`.\n",
    "Operations can be chained together, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T14:32:15.906470888Z",
     "start_time": "2023-11-01T14:32:15.633776731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2016-06-25 00:00:00\n",
      "ops: ['batch_dim', 'tensor']\n",
      "X_c: [torch.Size([1, 2, 52]), torch.Size([1, 2, 112])]\n",
      "Y_c: [torch.Size([1, 3, 52]), torch.Size([1, 1, 112])]\n",
      "X_t: [torch.Size([1, 2, 245])]\n",
      "Y_t: [torch.Size([1, 2, 245])]\n"
     ]
    }
   ],
   "source": [
    "print(task.add_batch_dim().convert_to_tensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gridded data in a `Task` can be flattened using the `.flatten_gridded_data` method.\n",
    "Notice how the `\"X\"` entries are now 2D arrays of shape `(2, M)` rather than tuples of two 1D arrays of shape `(M,)`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2016-06-25 00:00:00\n",
      "ops: ['gridded_data_flattened']\n",
      "X_c: [(2, 31161), (2, 30800)]\n",
      "Y_c: [(3, 31161), (1, 30800)]\n",
      "X_t: [(2, 245)]\n",
      "Y_t: [(2, 245)]\n"
     ]
    }
   ],
   "source": [
    "print(task_with_gridded_data.flatten_gridded_data())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:32:15.970618528Z",
     "start_time": "2023-11-01T14:32:15.909066194Z"
    }
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
